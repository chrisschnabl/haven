{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ef6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Union, List\n",
    "from pathlib import Path\n",
    "\n",
    "def read_parquet_file(file_path: Union[str, Path], columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a parquet file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the parquet file\n",
    "        columns: Optional list of columns to read\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the parquet data\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(file_path, columns=columns)\n",
    "\n",
    "\n",
    "file_path = \"./quantization_ablation_model/Meta-Llama-3-8B-Instruct.Q2_K.gguf/llama_classification.parquet\"\n",
    "df = read_parquet_file(file_path)\n",
    "print(f\"Loaded dataframe with shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO some classificaiton data I forgot to add the subject, so I'm going to add it now\n",
    "input_data_file_path = \"./input_datasets/classification_pairs.parquet\"\n",
    "input_data = read_parquet_file(input_data_file_path)\n",
    "def add_subject_column(df: pd.DataFrame, input_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add subject column to the dataframe by mapping from input_data.\n",
    "    \n",
    "    Args:\n",
    "        df: Target dataframe to add subject column to\n",
    "        input_data: Source dataframe containing subject information\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added subject column\n",
    "    \"\"\"\n",
    "    # Create mapping from question to subject\n",
    "    question_to_subject = dict(zip(input_data['question'], input_data['subject']))\n",
    "    # Map questions to subjects\n",
    "    df['subject'] = df['question'].map(question_to_subject)\n",
    "    return df\n",
    "\n",
    "df = add_subject_column(df, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c069e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff464ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"id\"] == 13594][\"response\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b99407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b977d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df: pd.DataFrame, group_by: str = None) -> pd.DataFrame:\n",
    "    if group_by:\n",
    "        accuracy_by_group = df.groupby(group_by)['correct'].mean().reset_index()\n",
    "        accuracy_by_group.columns = [group_by, 'accuracy']\n",
    "        \n",
    "        group_counts = df.groupby(group_by).size().reset_index(name='sample_count')\n",
    "        accuracy_by_group = accuracy_by_group.merge(group_counts, on=group_by)\n",
    "        \n",
    "        return accuracy_by_group\n",
    "    else:\n",
    "        overall_accuracy = df['correct'].mean()\n",
    "        return pd.DataFrame({'overall_accuracy': [overall_accuracy], 'sample_count': [len(df)]})\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = calculate_accuracy(df)\n",
    "print(f\"Overall accuracy: {overall_accuracy['overall_accuracy'].values[0]:.4f}\")\n",
    "\n",
    "# Calculate accuracy for only the answers that are in A, B, C, D\n",
    "valid_responses_df = df[df['response'].isin(['A', 'B', 'C', 'D'])]\n",
    "valid_responses_accuracy = calculate_accuracy(valid_responses_df)\n",
    "print(f\"\\nAccuracy for A, B, C, D responses only: {valid_responses_accuracy['overall_accuracy'].values[0]:.4f}\")\n",
    "print(f\"Sample count: {valid_responses_accuracy['sample_count'].values[0]}\")\n",
    "\n",
    "# Calculate accuracy by subject\n",
    "subject_accuracy = calculate_accuracy(df, group_by='subject')\n",
    "subject_accuracy = subject_accuracy.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nAccuracy by subject:\")\n",
    "print(subject_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "def load_classification_files(directory: str = \".\") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all classification result files from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory to search for classification files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping model names to their dataframes\n",
    "    \"\"\"\n",
    "    # Find all parquet files that might contain classification results\n",
    "    classification_files = glob.glob(os.path.join(directory, \"**\", \"*classification*.parquet\"), recursive=True)\n",
    "    \n",
    "    model_dfs = {}\n",
    "    for file_path in classification_files:\n",
    "        try:\n",
    "            # Extract model name from the file path\n",
    "            model_name = re.search(r'quantization_ablation_([^/\\\\]+)', file_path)\n",
    "            if model_name:\n",
    "                model_name = model_name.group(1)\n",
    "            else:\n",
    "                model_name = os.path.basename(file_path).replace('.parquet', '')\n",
    "            \n",
    "            # Load the dataframe\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Add model name as a column\n",
    "            df['model'] = model_name\n",
    "            \n",
    "            # Store in dictionary\n",
    "            model_dfs[model_name] = df\n",
    "            print(f\"Loaded {model_name} with {len(df)} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return model_dfs\n",
    "\n",
    "def combine_dataframes(model_dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine all model dataframes into a single dataframe.\n",
    "    \n",
    "    Args:\n",
    "        model_dfs: Dictionary of model dataframes\n",
    "        \n",
    "    Returns:\n",
    "        Combined dataframe\n",
    "    \"\"\"\n",
    "    if not model_dfs:\n",
    "        raise ValueError(\"No model dataframes provided\")\n",
    "    \n",
    "    return pd.concat(model_dfs.values(), ignore_index=True)\n",
    "\n",
    "def analyze_accuracy(combined_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and plot accuracy metrics across models.\n",
    "    \n",
    "    Args:\n",
    "        combined_df: Combined dataframe with all models\n",
    "    \"\"\"\n",
    "    # Calculate accuracy for each model\n",
    "    model_accuracy = combined_df.groupby('model')['correct'].mean().reset_index()\n",
    "    model_accuracy = model_accuracy.sort_values('correct', ascending=False)\n",
    "    \n",
    "    # Plot overall accuracy by model\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='model', y='correct', data=model_accuracy)\n",
    "    plt.title('Overall Accuracy by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy by subject for each model\n",
    "    subject_accuracy = combined_df.groupby(['model', 'subject'])['correct'].mean().reset_index()\n",
    "    \n",
    "    # Plot accuracy by subject for each model\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.barplot(x='subject', y='correct', hue='model', data=subject_accuracy)\n",
    "    plt.title('Accuracy by Subject and Model')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap of accuracy by subject and model\n",
    "    pivot_accuracy = subject_accuracy.pivot(index='subject', columns='model', values='correct')\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(pivot_accuracy, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5)\n",
    "    plt.title('Accuracy Heatmap by Subject and Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate valid response rate (A, B, C, D only)\n",
    "    valid_responses = combined_df.groupby('model').apply(\n",
    "        lambda x: x['response'].isin(['A', 'B', 'C', 'D']).mean()\n",
    "    ).reset_index(name='valid_response_rate')\n",
    "    \n",
    "    # Plot valid response rate\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='model', y='valid_response_rate', data=valid_responses)\n",
    "    plt.title('Valid Response Rate by Model (A, B, C, D only)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Valid Response Rate')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_timing(combined_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and plot timing metrics across models.\n",
    "    \n",
    "    Args:\n",
    "        combined_df: Combined dataframe with all models\n",
    "    \"\"\"\n",
    "    # Check if timing columns exist\n",
    "    timing_cols = ['prompt_time', 'response_time']\n",
    "    if not all(col in combined_df.columns for col in timing_cols):\n",
    "        print(\"Timing columns not found in the dataframe\")\n",
    "        return\n",
    "    \n",
    "    # Calculate total time\n",
    "    combined_df['total_time'] = combined_df['prompt_time'] + combined_df['response_time']\n",
    "    \n",
    "    # Calculate average timing metrics for each model\n",
    "    timing_metrics = combined_df.groupby('model')[timing_cols + ['total_time']].mean().reset_index()\n",
    "    \n",
    "    # Plot average timing metrics\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    timing_metrics_melted = pd.melt(\n",
    "        timing_metrics, \n",
    "        id_vars=['model'], \n",
    "        value_vars=timing_cols + ['total_time'],\n",
    "        var_name='Timing Metric', \n",
    "        value_name='Time (seconds)'\n",
    "    )\n",
    "    sns.barplot(x='model', y='Time (seconds)', hue='Timing Metric', data=timing_metrics_melted)\n",
    "    plt.title('Average Timing Metrics by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Timing Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot timing distribution with boxplots\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    timing_data_melted = pd.melt(\n",
    "        combined_df, \n",
    "        id_vars=['model'], \n",
    "        value_vars=timing_cols + ['total_time'],\n",
    "        var_name='Timing Metric', \n",
    "        value_name='Time (seconds)'\n",
    "    )\n",
    "    sns.boxplot(x='model', y='Time (seconds)', hue='Timing Metric', data=timing_data_melted)\n",
    "    plt.title('Timing Distribution by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Timing Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate tokens per second for response generation\n",
    "    if 'response_tokens' in combined_df.columns:\n",
    "        combined_df['tokens_per_second'] = combined_df['response_tokens'] / combined_df['response_time']\n",
    "        \n",
    "        # Plot tokens per second by model\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(x='model', y='tokens_per_second', data=combined_df)\n",
    "        plt.title('Response Generation Speed (Tokens per Second) by Model')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Tokens per Second')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def analyze_error_patterns(combined_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze error patterns across models.\n",
    "    \n",
    "    Args:\n",
    "        combined_df: Combined dataframe with all models\n",
    "    \"\"\"\n",
    "    # Filter for incorrect responses only\n",
    "    incorrect_df = combined_df[combined_df['correct'] == False]\n",
    "    \n",
    "    # Count frequency of each incorrect response by model\n",
    "    error_patterns = incorrect_df.groupby(['model', 'response']).size().reset_index(name='count')\n",
    "    \n",
    "    # Plot error patterns\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for model in error_patterns['model'].unique():\n",
    "        model_errors = error_patterns[error_patterns['model'] == model]\n",
    "        model_errors = model_errors.sort_values('count', ascending=False).head(10)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='response', y='count', data=model_errors)\n",
    "        plt.title(f'Top 10 Error Responses for {model}')\n",
    "        plt.xlabel('Response')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main analysis function\n",
    "def run_classification_analysis() -> None:\n",
    "    \"\"\"Run comprehensive analysis on classification results.\"\"\"\n",
    "    # Load all classification files\n",
    "    model_dfs = load_classification_files()\n",
    "    \n",
    "    if not model_dfs:\n",
    "        print(\"No classification files found.\")\n",
    "        return\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = combine_dataframes(model_dfs)\n",
    "    \n",
    "    # Run analyses\n",
    "    print(\"\\n=== Accuracy Analysis ===\")\n",
    "    analyze_accuracy(combined_df)\n",
    "    \n",
    "    print(\"\\n=== Timing Analysis ===\")\n",
    "    analyze_timing(combined_df)\n",
    "    \n",
    "    print(\"\\n=== Error Pattern Analysis ===\")\n",
    "    analyze_error_patterns(combined_df)\n",
    "\n",
    "# Run the analysis\n",
    "run_classification_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb7adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
